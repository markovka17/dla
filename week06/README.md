# Week 06

* [Lecture slides](https://docs.google.com/presentation/d/1OoL2XYuBId9XVMXpoKzzDhOYUhQ6j-HWb1qYuQLoxJ8/edit?usp=sharing)
* [Recording on YouTube (in Russian)](https://youtu.be/mNkwO8f3Edk)


### Keywork Spoting Models
* [DNN based KWS](https://static.googleusercontent.com/media/research.google.com/ru//pubs/archive/42537.pdf), [CNN based KWS](https://www.dropbox.com/s/peymtgllizb0ysq/KWS_CNN.pdf?dl=0)
* [Attention based KWS](https://www.dropbox.com/s/22ah2ba7dug6pzw/KWS_Attention.pdf?dl=0)
* [About Orthogonality Constraint for KWS](https://arxiv.org/abs/1910.04500)
* [SVDF-1](https://www.dropbox.com/s/ek1khslpvbledtu/SVDF.pdf?dl=0), [SVDF-2](https://www.dropbox.com/s/i2z5hrk81uhv7pr/SVDF2.pdf?dl=0)
* [Focal Loss](https://arxiv.org/pdf/1708.02002.pdf)
* [A lot of useful stuff about NN compression](https://nervanasystems.github.io/distiller/schedule.html#compression-scheduler)
* [Broadcasted Residual Learning](https://arxiv.org/pdf/2106.04140v2.pdf)
* [SubSpectral Normalization](https://arxiv.org/pdf/2103.13620.pdf)
* [Keyword Transformer: A Self-Attention Model for Keyword Spotting](https://arxiv.org/pdf/2104.00769.pdf)
* [Temporal Convolution for Real-time Keyword Spotting on Mobile Devices](https://arxiv.org/pdf/1904.03814v2.pdf)

### Quantization
* [Quantizing deep convolutional networks for efficient inference: A whitepaper](https://arxiv.org/pdf/1806.08342.pdf)
* [A Survey of Quantization Methods](https://arxiv.org/pdf/2103.13630.pdf)

### Compression and speed up of neural networks
* [CNN prunning](https://jacobgil.github.io/deeplearning/pruning-deep-learning)
* [Knowledge Distilation](https://arxiv.org/pdf/1503.02531.pdf)
* [A Survey of Knowledge Distilation](https://arxiv.org/pdf/2006.05525.pdf)
